
<nav>
			<a href="/Elections_Project/">Home</a>
	        	<a href="/Elections_Project/Motivation">Motivation</a>
        		<a href="/Elections_Project/EDA">Exploratory Data Analysis</a>
			<a href="/Elections_Project/Code">Code</a>
        		<a href="/Elections_Project/Appendix">Appendix</a>
		</nav>

# Motivation: 


<center>
Figure 1: "Dewey Defeats Truman" (1948)

![dewey_truman](dewey_truman.jpg)
</center>

Ever since competitive elections have existed, people have tried to predict them. And with good reason -- elections determine the people who will run a town, state, or country for a significant amount of time, and gives those elected the reins of the state to use as they wish. 

Knowing what's at stake, the general public, including heads of households and business leaders look to plan for proposed tax hikes, safety regulations, etc. Having an idea of what the new government will look like before it's set in stone thus allows for a calmer transition between governments. [When parties were less ideological](https://catalog.hathitrust.org/Record/001749434) (such as in Figure 1 above, where president-elect Harry Truman holds up the *Chicago Tribune* [inaccurately] announcing his defeat), predicting elections was interesting but perhaps not extremely consequential, since either party was expected to promote the same kinds of policies. In the increasingly divided and polarized U.S., such questions of government composition become essential. 

Newspapers and universities took up the task of polling voters in both [in their communities](https://law.marquette.edu/poll/) and [across the country](https://poll.qu.edu/). While these polls have played an essential role in drawing up predictions, some polls may be biased due to their participant selection mechanism or question wording. 

When a survey firm uses random-digit dialing, potentially-valid phone numbers are generated by the survey firm at random and called to recruit potential participants. While this method worked well in the past, when everyone had a phone and answered nearly every time, the advent of caller ID meant that fewer people picked up a phone call from a number they did not recognize, and those who did likely constituted a non-random, biased sample of all voters ([Green & Gerber 2002](https://academic.oup.com/poq/article-abstract/70/2/197/1912498)). This non-random sampling makes the poll less reliable as a signal for the actual outcome of the election.

Similarly, different polls may ask what is essentially the same question in different manners. [Research has shown](https://doi.org/10.1086/269158) that the way in which a question is posed to the survey participant affects the respondent's answer. For example, a registered Republican voter, when asked whether she supports John Doe (Dem.) in the governor's race, may answer yes, since she supports his stance on support for young parents. That same respondent, if she is asked whether she supports the Democrat in the governor's race, may think first about her partisan affiliation and second about the actual candidate, and respond no. Thus, given multiple polls whose question wording differs are very difficult to aggregate into any coherent picture of the state of the country.

In the lead-up to the 2008 election, [news media](https://www.nytimes.com/section/upshot) and [newly-minted forecasting agencies](https://fivethirtyeight.com/) began to use statistics to assess polls' accuracy (by predictive power and with reference to other polls). Using a Bayesian approach, the researchers used the prior election result in the district as a prior, and also included information about the survey design (participant selection, question wording, target audience, sample size etc.) to weight the trustworthiness of each poll and ultimately to aggregate them into an overall prediction for each Congressional district (*N* = 435). 

However, the results of the 2016 election did not match those predicted by most analysts. 
Following the 2016 election, the data analytics firm 538 published a [post-mortem](https://fivethirtyeight.com/features/the-real-story-of-2016/) on what went wrong in the production and mass interpretation of their and others' models. 

<center>
Figure XXX: New York Times' *Upshot* Election Forecasting Graphic

![](nytimes_2016_map.png "New York Times Election Forecast Visualization, 2016")
</center>

Figure XXX shows the user interface of the New York Times' election forecast visualizer as it was presented prior to the 2016 election (though not using the predictions the Times' team generated). In the map, each state is presented with its abbreviation and the number of votes it has in the electoral college, the formal body that technically elects the president. Geographically smaller states are presented in the boxes on the right, and states that are allowed to split their electoral college vote between candidates, Maine and Nebraska, are listed in the bottom right. 

The *Times*, as well as several [others](https://www.cookpolitical.com/ratings/house-race-ratings), distinguished the degrees of partisanship here: Districts could be "Strong" Democrat/Republican, "Likely" Democrat/Republican, "Lean" Democrat/Republican, or a toss-up. While this method clearly distinguishes what's likely an uncompetitive race in New York (Strong Democrat), the underlying differences between these groups are opaque to the reader. By obscuring the underlying data and the uncertainty of the predictions that can be made using it, the map does more harm than good. 

Therefore, in this project, we make it a priority to emphasize the uncertainty in our model. It is highly unlikely that we will be able to accurately predict all districts, but it is likely less costly to know that the race is uncertain, than to predict the wrong outcome entirely.

